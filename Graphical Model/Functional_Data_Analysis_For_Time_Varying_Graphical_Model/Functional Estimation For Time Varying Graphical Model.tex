%----------------------------------------------------------------------------------------
%	DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[11pt]{article}

\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cleveref}

\RequirePackage{amsmath}
\RequirePackage{amssymb}
\RequirePackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{mathrsfs}

\usepackage{epsfig}
\usepackage{amscd}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{enumerate}
\usepackage{tikz}


\usepackage[top=1.5in,bottom=1.5in,right=1.5in, left=1.5in]{geometry}

\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}
\renewcommand\thesubfigure{(\alph{subfigure})}


%new command
%% Blackboard bold letters
\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}

%% Bold font letters
\newcommand{\bX}{\mathbf X}
\newcommand{\bY}{\mathbf Y}
\newcommand{\bA}{\mathbf A}
\newcommand{\bB}{\mathbf B}
\newcommand{\bM}{\mathbf M}
\newcommand{\bI}{\mathbf I}
\newcommand{\bU}{\mathbf U}

%% bold Greek symbols
\newcommand{\balpha}{{\boldsymbol{\alpha}}}
\newcommand{\bbeta}{{\boldsymbol{\beta}}}
\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\bdeta}{{\boldsymbol{\eta}}}
\newcommand{\btheta}{{\boldsymbol{\theta}}}
\newcommand{\bseta}{\boldsymbol{\eta}}
\newcommand{\brho}{{\boldsymbol{\rho}}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\bvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}

\newcommand{\bOmega}{{\boldsymbol{\Omega}}}
\newcommand{\bSigma}{{\boldsymbol{\Sigma}}}
\newcommand{\bTheta}{{\boldsymbol{\Theta}}}



%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------



%----------------------------------------------------------------------------------------
%	ARTICLE SECTION
%----------------------------------------------------------------------------------------

\begin{document}

\author{Meilei Jiang, Yufeng Liu\\
    Department of Statistics and Operations Research\\
		University of North Carolina at Chapel Hill}
\title{Functional Sparse Estimation of Time Varying Graphical Model}

\maketitle

\section{Introduction}

Graphical models are quite useful in many domains to uncover the dependence structure among observed variables. Typically, we consider a $p$-dimensional multivariate normal distributed random variable $ \bX = (X_1, \cdots, X_p) \sim \N(\bmu, \bSigma) $, where $p$ is the number of features. Then a useful graph of these $p$ features can be constructed based on there conditional dependence structure. More precisely, we can construct a Gaussian graphical model $\mathcal{G} = (V, E)$, where $V = \{ 1, \cdots, p\}$ is the set of nodes and $E = \{ (i, j) | X_i \text{ is conditionally dependent with } X_b, \text{given } X_{V/\{i, j\}}\}$.     

Let $\bOmega = \bSigma^{-1} = (\omega_{i,j})_{1\leq i, j \leq p}$ be the precision matrix. Then $X_i$ and $X_j$ are conditionally dependent given other features if and only if $\omega_{ij} = 0$. Therefore, estimating the covariance matrix and precision matrix of $X$ is equivalent to estimate the structure of Gaussian graphical model $\mathcal{G}$. More discussion can be found in \cite{lauritzen1996graphical}.

There are lots of literatures discussing about estimating $\bSigma$ and $\bOmega$. Utilizing the idea of LASSO from Tibshirani~\cite{tibshirani1996regression}, Meinshausen and B{\"u}hlmann~\cite{meinshausen2006high} used a regression approach, which is a computationally attractive method, to perform the nearest neighborhood selection of non-zero elements of $\bOmega$ at each node in the graph. Another nature way is to estimate $\bSigma$ and $\Omega$ is the penalized likelihood approach. Friedman, Hastie nd Tibshirani~\cite{friedman2008sparse} proposed the graphical lasso. Fan, Feng and Wu~\cite{fan2009network} studied the penalized likelihood methods with the SCAD penalty and the adaptive LASSO penalty. Cai, Liu and Luo~\cite{cai2011constrained} performed a constrained $l_1$ minimization approach to estimate sparse precision matrix (CLIME). 

In particular, we are interested in estimating multiple graphs from different locations or time points. Peng et al.~\cite{peng2012partial} proposed the regression approach to estimate multiple graphs through an active-shooting algorithm. Danaher et al.~\cite{danaher2014joint} propose the \emph{joint graphical lasso}, to estimate multiple graphical models corresponding to distinct but related conditions. The \emph{joint graphical lasso} utilized fussed lasso and group lasso to force similarity among graphs. These approaches generated estimation of similar graphs by forcing the similarity of parameters.

Zhou, Laffery and Wasserman~\cite{zhou2010time} developed a nonparametric framework for estimating time varying graphical model for estimating time varying graphical structure for multivariate Gaussian distributions $\bX^t \sim \N(0, \Sigma(t))$ using $ l_1 $ regularization method. Zhou's model assumed that the observations $X^t$ are independent and changed smoothly. Kolar and Xing~\cite{kolar2011time} showed the model selection consistency for the procedure proposed in Zhou et al.~\cite{zhou2010time} and for the modified neighborhood selection procedure of Meinshausen and B{\"u}hlmann~\cite{meinshausen2006high}. Lu, Kolar and Liu~\cite{lu2015post} proposed a dynamic nonparanormal graphical model, which is more robust, by estimating a weighted Kendall's tau correlation matrix. These approaches generated estimation of similar graphs by weighting the samples among different times.

All these methods mentioned above can only estimate multiple graphs on several fix points. If we are interested in estimating a dynamic graph which is smoothing over a time region, a functional model which consider each feature as functional data over time is very attractive. Since the structure of Gaussian graph can be recovered though regression approach, we can consider dynamic graphical model under the context of varying coefficient model~\cite{hastie1993varying, fan1999statistical, fan2008statistical}. Typically, we are interested in a nonparametric approach to estimate the varying coefficient models in Huang, Wu and Zhou's paper~\cite{huang2002varying}. 
$$ Y(t) = X(t)^T \bbeta(t) + \varepsilon(t) $$
Then we are looking at the following model
$$ X_i(t) = \bX_{-i}^T(t) \bbeta_i(t) + \varepsilon_i(t) = \sum_{\substack{j\neq i}} X_j(t) \beta_{ij}(t) + \varepsilon_i(t). $$
Moreover, in order to estimate a sparse graph, we need to gain sparse functional coefficient $\bbeta(t)$. Based on the idea of FLiRTI in James, Wang and Zhu's paper~\cite{james2009functional}, we put penalty on the derivative matrices of $\bbeta(t)$.  


\section{Methodology}

\subsection{Functional Undirected Graph}
Consider $p$ smooth functional continuous variables $\{ X_1(t), X_2(t) \cdots, X_p(t)\}$ on the 'time' domain $\mathcal{T}$. On each $t$, we assume
$$ (X_1(t), X_2(t) \cdots, X_p(t)) \sim \N(\mathbf{0}, \bSigma(t)). $$ 
Define the undirected graph 
\begin{equation}
	\label{eq:funcgraph}
	\begin{aligned}
	G(t) &= \Big\{ V, E(t) \Big\},\text{ where } V = \Big\{1,\cdots, p\Big\},\\
	\text{ and } E(t) &= \Big\{ (i,j) \in V^2 : \text{Cov} \Big[ X_i(t), X_j(t)| X_k(t), k \neq i,j \Big] \neq 0, i \neq j \Big\}.\\
	\end{aligned}
\end{equation}

Namely, $G(t)$ is the Gaussian graphical model at each $t$. This model is quite flexible, which allows $G(t)$ evolve over time and includes the time dependence. Assume that data are observed at $t_1, \cdots, t_n$ and at each time point $t$ we have $n_{t}$ samples.

\subsection{Functional Nearest Neighborhood Selection}

Consider the following functional linear model
\begin{equation} 	\label{eq:flm}
    \begin{aligned}
    X_i^r(t) &= {\bX_{-i}^r(t)}^T \bbeta_i(t) + \varepsilon_i^r(t) = \sum_{\substack{j\neq i}} X_j^r(t) \beta_{ij}(t) + \varepsilon_i^r(t),\\
    \text{ where }\bX_{-i}^r(t) &= (X_j^r(t))_{\substack{j \neq i}} \in \R^{(p-1) \times 1}, r = 1, \cdots, n_t, t = t_1, \cdots, t_n, i = 1, \cdots, p.
    \end{aligned}
\end{equation}
For each functional coefficient $\beta_{ij}(t)$, we consider the basis $\bB_{ij}(t) = (B_{ij1}(t), \cdots, B_{ijk_{ij}}(t) )$ and then 
$$\beta_{ij}(t) = \sum_{\substack{s=1}}^{k_{ij}} B_{ijs}(t) \gamma_{ijs} + e_{ij}(t) = \bB_{ij}(t) \bgamma_{ij} $$
Thus Equation~(\ref{eq:flm}) can be represented as
\begin{equation}
	\label{eq:flm2}
	\begin{aligned}
	X_i^r(t) &= \sum_{\substack{j \neq i}} \sum_{\substack{s=1}}^{k_{ij}} X_j^r(t) B_{ijs}(t) \gamma_{ijs} + \tilde{\varepsilon}^r(t), \\
	\text{where } \tilde{\varepsilon}^r(t) &= \sum_{\substack{j \neq i}} X_j^r(t)e_{ij}^r(t) + \varepsilon^r(t), r = 1, \cdots, n_t, t = t_1, \cdots, t_n, i = 1, \cdots, p.\\
	\end{aligned}
\end{equation}
As seen in Equation~(\ref{eq:flm2}), our model is quite flexible since the basis of each functional coefficient can be different.

To get the matrix form of Equation~(\ref{eq:flm2}), denote 
\begin{align*}
	&\bB(t) = \text{diag}\{\bB_{ij}(t)\} \in \R^{(p-1)\times \sum_{\substack{j \neq i}}k_{ij} },\\
	&\bU_i^r(t) = \bB(t)^T X_{-i}^r(t) \in \R^{ \sum_{\substack{j \neq i}}k_{ij} \times 1},\\
	&\bU_i(t) = ( \bU_i^1(t), \cdots, \bU_i^{n_t}(t) )^T \in \R^{n_t \times \sum_{\substack{j \neq i}}k_{ij}},\\
	&\bU_i = ( \bU_i(t_1), \cdots, \bU_i(t_n) )^T \in \R^{\sum_{t = 1}^{n}n_t \times \sum_{\substack{j \neq i}}k_{ij}},\\
	&\bX_i(t) = (X_i^1(t), \cdots, X_i^{n_t}(t))^T \in \R^{n_t \times 1},\\
	&\bX_i = (X_i(t_1), \cdots, X_i(t_n))^T \in \R^{\sum_{t = 1}^{n}n_t \times 1},\\
	&\bvarepsilon_i(t) = (\varepsilon_i^1(t), \cdots, \varepsilon_i^{n_t}(t))^T \in \R^{n_t \times 1},\\
	&\bvarepsilon_i = (\varepsilon_i(t_1), \cdots, \varepsilon_i(t_n))^T \in \R^{\sum_{t = 1}^{n}n_t \times 1},\\
	&\bgamma_i = (\bgamma_{ij})_{j \neq i} \in \R^{\sum_{\substack{j \neq i}}k_{ij} \times 1}.
\end{align*}

Then the Equation~(\ref{eq:flm2}) can be expressed as
\begin{equation}
	\label{eq:flm3}
	\bX_i = \bU_i \bgamma_i + \bvarepsilon_i, i = 1, \cdots, p.
\end{equation}
 
\subsection{Control The Sparsity Of Derivatives}

In Model~(\ref{eq:flm3}), we want to estimate a sparse graph and interpretable coefficient functions. For each $i$ and $j \neq i$, we want to control the sparsity of $ \beta_{ij}^{(m)} = \frac{\mathrm{d^m}}{\mathrm{d} t^m} \beta_{ij}(t) \approx \frac{\mathrm{d^m}}{\mathrm{d} t^m} \bB_{ij}(t)^T \bgamma_{ij}$ for some $m$. Say assume $\beta_{ij}^{(0)}(t) = 0$ and $\beta_{ij}^{(2)}(t) = 0$ in large area, then $\beta_{ij}(t)$ is zero in many region and linear in the left regions.

Let 
\begin{equation}
	\bA_{ij} = \begin{bmatrix} D^m\bB_{ij}(t_1) & \cdots & D^m\bB_{ij}(t_n) \end{bmatrix}^T \in \R^{n \times k_{ij}},
\end{equation} 
where $D^m$ is the $m$th finite difference operator, i.e., $D\bB_{ij}(t_k) = [\bB_{ij}(t_k) - \bB_{ij}(t_{k-1})]/[t_k - t_{k-1}], D^2\bB_{ij}(t_k) = [D\bB_{ij}(t_k) - D\bB_{ij}(t_{k-1})]/[t_k - t_{k-1}]$, etc. 

Next, set
\begin{equation}
	\bseta_{ij} = \bA_{ij} \bgamma_{ij} \in \R^{n \times 1}  
\end{equation}
Then $\bseta_{ij} \approx (\beta_{ij}^{(m)}(t_k))_{1 \leq k \leq n}$. Moreover, we denote 
\begin{equation}
	\begin{aligned}
	\bseta_i &= (\bseta_{ij})_{j \neq i} = \bA_i \bgamma_i \in \R^{n(p-1)},\\
    \text{where }\bA_i&= \text{diag}(\bA_{ij})_{j \neq i} \in \R^{n(p-1)  \times \sum_{\substack{j \neq i}} k_{ij} }.
	\end{aligned}
\end{equation}
We want to put the sparsity penalty on the $\bseta_i$. Then Model~\ref{eq:flm3} can be expressed as the following optimization problem, which is a generalized lasso problem. 
%In order to incorporate $\bseta_{ij}$ into the model~\ref{eq:flm3}, we need to represent $\bgamma_{ij}$ as a transformation of $\bseta_{ij}$. 



%When $\bA_{ij}$ is full row rank (i.e. $k_{ij} \geq n$), there exists an upper triangle matrix $\bM_{ij} \in \R^{n \times n}$, such that $\bM_{ij} \bA_{ij} = (\bI_n, \mathbf{0})$. Similarly, we denote the first $n$ rows of $\bgamma_{ij}$ as $\bgamma_{ij}^{(1)}$, and the left as $\bgamma_{ij}^{(2)}$. Thus, $\bgamma_{ij}^{(1)} = \bM_{ij} \bseta_{ij}$ and $\bgamma_{ij} = (\bM_{ij} \bseta_{ij}, \bgamma_{ij}^{(2)})$. 

%When $\bA_{ij}$ is full column rank (i.e. $k_{ij} < n$), we can also find the pseudo inverse matrix $\bM_{ij} =(A^TA)^{-1}A^T  \in \R^{n \times n}$, such that, $\bM_{ij} \bA_{ij} = (\bI_{k_{ij}}, \mathbf{0})^T$. And we denote the first $k_{ij}$ rows of $\bM_{ij}$ as $\bM_{ij}^{(1)}$, and the left as $\bM_{ij}^{(2)}$. Then $\bgamma_{ij} = \bM_{ij}^{(1)}\bseta_{ij}$.

%To be concise, we denote $\bgamma_{ij} = \bM_{ij}^* \bseta_{ij}^* = \begin{cases}
%\begin{bmatrix}
%\bM_{ij}  & \mathbf{0} \\
%\mathbf{0} & \bI_{k_{ij} - n} \\
%\end{bmatrix} \begin{bmatrix}
%\bseta_{ij} \\
%\bgamma_{ij}^{(2)} \\
%\end{bmatrix}      & \quad \text{if } k_{ij} \geq n, \\
%\bM_{ij}^{(1)}\bseta_{ij}  & \quad \text{otherwise. }\\
%\end{cases}$. Then we have 
%\begin{equation}
%	\begin{aligned}
%	&\bgamma_i = \bM_i^* \bseta_i^*, \\
%	&\text{where } \bM_i^* = \text{diag} \{\bM_{ij}^*\}_{j \neq i} \in \R^{(\sum_{\substack{j \neq i}} k_{ij}) \times (\sum_{\substack{j \neq i}} k_{ij}^*)},\\ 
%	&\bseta_i^* = (\bseta_{ij}^*)_{j \neq i} \in \R^{(\sum_{\substack{j \neq i}} k_{ij}^*) \times 1} \text{and } k_{ij}^* = \max (k_{ij}, n)\\
%	\end{aligned}
%\end{equation}    
\begin{equation}
    \label{eq:glars}
    \begin{aligned}
    \hat{\bgamma}_{i,L} &= \text{arg} \min_{\substack{\bgamma_i}} \frac{1}{2}\|\bX_i - \bU_i \bgamma_i\|_2^2 \\
    \text{subject to } \|\bseta_i\|_1 &= \|\bA_i \bgamma_i\|_1 \leq t\\
    \text{i.e. }\hat{\bgamma}_{i,L} &= \text{arg} \min_{\substack{\bgamma_i}} \frac{1}{2}\|\bX_i - \bU_i \bgamma_i\|_2^2 + \lambda \|\bA_i \bgamma_i\|_1\\
    \end{aligned}
\end{equation}

Moreover, if we want to control the sparsity of multiple derivatives of $\bbeta_{ij}(t)$, say we want both $\bbeta_{ij}^{(0)}(t) = 0$ and $\bbeta_{ij}^{(2)}(t) = 0$ in large area.

There is a connection between Model~\ref{eq:glars} and fused lasso.

\bibliographystyle{plain}
\bibliography{dynamicgraph}

\end{document}
